{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes import client, config, watch\n",
    "from k8scontroller.kubernetes_job import kube_create_job\n",
    "from k8scontroller.kubernetes_job import kube_test_credentials, kube_cleanup_finished_jobs, kube_delete_empty_pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data to/from AFS\n",
    "\n",
    "## README\n",
    "\n",
    "Set your AFS_NAME, AFS_KEY and AFS_SHARE envs\n",
    "\n",
    "Specify your \"local_root\" directory where to download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"AFS_NAME\"] = 'kubeocean'\n",
    "os.environ[\"AFS_KEY\"] =  'g73h7V65FWE7xYVyzdaGs3ec3M7Yig0hlc9u2oSpPh1N+G9mVpB//Z445aHvqekUejn93Hu237TRYqGhqGOW4w=='\n",
    "\n",
    "# os.environ[\"AFS_NAME\"] = <AFS_NAME>\n",
    "# os.environ[\"AFS_KEY\"] =  <AFS_KEY>\n",
    "os.environ[\"AFS_SHARE\"] = 'datalake'\n",
    "\n",
    "os.environ[\"AZURE_SECRET\"] = \"azure-secret\"\n",
    "os.environ[\"AFS_VOLUME_NAME\"] = \"azure-volume\"\n",
    "os.environ[\"AZURE_MOUNT_PATH\"]= \"/input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Rewriting data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file_3_3.txt [==================================================]\n",
      "Uploading file_1_1.txt [==================================================]\n",
      "Uploading file_2_2.txt [==================================================]\n",
      "Directory /home/igor/Repository/local_root/2019-05-06-18.52-68b1f1fdbc was rewritten \n",
      "Downloading file_1_1.txt [==================================================]\n",
      "Downloading file_2_2.txt [==================================================]\n",
      "Downloading file_3_3.txt [==================================================]\n"
     ]
    }
   ],
   "source": [
    "from k8scontroller.data_loader import AFSLoader\n",
    "\n",
    "\n",
    "local_root=<local_root>\n",
    "data_folder=<local_data_folder>\n",
    "afs_loader = AFSLoader(local_root=local_root)\n",
    "afs_path = afs_loader.upload_data_afs(data_folder)\n",
    "afs_loader.download_data_afs(afs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-141910ea465a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mkube_worker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKubeWorker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mapi_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkube_worker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkube_create_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"busybox\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/LAMBDA/kubernetes-job-controller/k8scontroller/kubernetes_job.py\u001b[0m in \u001b[0;36mkube_create_job\u001b[0;34m(self, container_image, command, volume_sub_path, env_vars)\u001b[0m\n\u001b[1;32m     42\u001b[0m                                            \u001b[0mcommand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                                            \u001b[0mvolume_sub_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvolume_sub_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                                            env_vars=env_vars)\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             api_response = self.api_instance.create_namespaced_job(\"default\", body,\n",
      "\u001b[0;32m~/LAMBDA/kubernetes-job-controller/k8scontroller/kubernetes_job.py\u001b[0m in \u001b[0;36mkube_create_job_object\u001b[0;34m(self, container_image, command, volume_sub_path, namespace, env_vars)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Body needs Metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# Attention: Each JOB must have a different name!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV1ObjectMeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;31m# And a Status\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV1JobStatus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'name' is not defined"
     ]
    }
   ],
   "source": [
    "from k8scontroller.kubernetes_job import KubeWorker\n",
    "\n",
    "kube_worker = KubeWorker()\n",
    "api_response = kube_worker.kube_create_job(\"busybox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cleaning\n",
      "Jobs were Cleaned\n"
     ]
    }
   ],
   "source": [
    "kube_test_credentials()\n",
    "print(\"Starting cleaning\")\n",
    "kube_cleanup_finished_jobs()\n",
    "print(\"Jobs were Cleaned\")\n",
    "kube_delete_empty_pods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import Optimizer\n",
    "from skopt.learning import GaussianProcessRegressor\n",
    "from skopt.learning.gaussian_process.kernels import RBF, ConstantKernel, Product\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from skopt import gp_minimize\n",
    "from time import sleep\n",
    "import docker\n",
    "import random\n",
    "import os\n",
    "import string\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# первые n_initial_points модель не обучается\n",
    "n_initial_points = 5\n",
    "\n",
    "# число итераций цикла\n",
    "n_calls = 3\n",
    "\n",
    "# оптимизация на кубе [low_constraint, high_constraint]^dim\n",
    "low_constraint, high_constraint = 2., 301.\n",
    "dim = 1\n",
    "\n",
    "# столько контейнеров вызываются для параллельной работы\n",
    "batch_size = 2\n",
    "\n",
    "# директория на сервере, хранит директории, которые будут монтироваться в контейнеры\n",
    "folder_local = '/home/matyushinleonid/lhcb_ecal/feb_meeting/folder_local'\n",
    "folder_local = '/home/igor/LAMBDA/lhcb_repo'\n",
    "ptint(\"Ваш путь до директории с данными {} \".format(folder_local))\n",
    "\n",
    "# директория для файлов input и output внутри контейнера\n",
    "folder_container = '/home/nb_user/logs'\n",
    "\n",
    "# python-клиент докера\n",
    "client = docker.from_env()\n",
    "\n",
    "# имя образа\n",
    "image = \"calorbuild\"\n",
    "\n",
    "# имена директорий, каждая соответствует своей копии образа\n",
    "worker_names = ['first_worker', 'second_worker']\n",
    "\n",
    "###\n",
    "first_loop_legal_upper_bounds = [i for i in range(3, 301, 3)]\n",
    "#second_loop_legal_upper_bounds = [i // 3 * 4 for i in first_loop_legal_upper_bounds]\n",
    "#space_size = len(first_loop_legal_upper_bounds)\n",
    "#total_amount_of_inner_part = [first_loop_legal_upper_bounds[i] * second_loop_legal_upper_bounds[i] \\\n",
    "#                              for i in range(space_size)]\n",
    "\n",
    "def crop_number(n):\n",
    "    return min(first_loop_legal_upper_bounds, key=lambda t:abs(t-n))\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = Product(ConstantKernel(1), RBF(1)) + ConstantKernel(1)\n",
    "\n",
    "model = GaussianProcessRegressor(alpha=0, \n",
    "                                 normalize_y=True, \n",
    "                                 noise='gaussian', \n",
    "                                 n_restarts_optimizer=10, \n",
    "                                 kernel=kernel)\n",
    "\n",
    "optimizer = Optimizer([[low_constraint, high_constraint]]*dim,\n",
    "                      model,\n",
    "                      n_initial_points=n_initial_points,\n",
    "                      acq_func='EI',\n",
    "                      acq_optimizer='lbfgs',\n",
    "                      random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folder(folder_local):\n",
    "    list_dir = os.listdir(folder_local)\n",
    "    for _ in range(3):\n",
    "        new_folder ='{}/{}'.format(folder_local,\n",
    "                                   ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(10)))\n",
    "        if new_folder not in list_dir:\n",
    "            os.mkdir(new_folder)\n",
    "            return new_folder\n",
    "    raise Exception(\"Cannot create uniq folder\")   \n",
    "\n",
    "\n",
    "def write_input_file(input_data):\n",
    "    job_folder = get_folder(folder_local)\n",
    "    file_to_write = '{}/input.txt'.format(job_folder)\n",
    "    string_to_write = '\\n'.join(map(str, input_data))\n",
    "    with open(file_to_write, \"w\") as file:\n",
    "        print(string_to_write,\n",
    "              file=file)\n",
    "    return job_folder\n",
    "\n",
    "def new_point(x):\n",
    "    croped_x = min(first_loop_legal_upper_bounds, key=lambda t:abs(t-x))\n",
    "    return (croped_x, croped_x // 3 * 4) \n",
    "\n",
    "def create_job(job_folder, **kwargs):\n",
    "    client.containers.run(privileged=True,\n",
    "                          remove=False,\n",
    "                          detach=False,\n",
    "                          hostname='dev',\n",
    "                          tty=True,\n",
    "                          stdin_open=True,\n",
    "                          volumes={job_folder: {'bind': folder_container,\n",
    "                                                     'mode': 'rw'}},\n",
    "                          **kwargs)\n",
    "    \n",
    "\n",
    "def read_output_file(job_folder):\n",
    "    file_to_read = '{}/output.txt'.format(job_folder)\n",
    "    with open(file_to_read, 'r') as myfile:\n",
    "        data = myfile.read()\n",
    "    return float(data)\n",
    "\n",
    "def get_price(params, lamb=1):\n",
    "    param1 = crop_number(params[0])\n",
    "    param2 = param1 // 3 * 4\n",
    "    return lamb * param1 * param2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Queue, Pool, Manager\n",
    "import traceback\n",
    "import logging\n",
    "\n",
    "def test_worker(q_in, q_out):\n",
    "    while True:         \n",
    "        try:\n",
    "            data = q_in.get()            \n",
    "            in_dir = write_input_file(data)\n",
    "            logging.info('Start Job {}'.format(in_dir))\n",
    "            create_job(in_dir, \n",
    "                       image='busybox', \n",
    "                       command=\"/bin/sh -c 'head -1 input.txt > output.txt'\",\n",
    "                       working_dir='/home/nb_user/logs')  \n",
    "            result = read_output_file(in_dir)\n",
    "            q_out.put(( data, result ))   \n",
    "        except:\n",
    "            logging.error(\"Unexpected error:\", sys.exc_info()[0])\n",
    "            logging.error(traceback.format_exc())\n",
    "            raise\n",
    "        logging.info('Job {} is done'.format(in_dir))\n",
    "    return   \n",
    "\n",
    "def worker(q_in, q_out):\n",
    "    while True:         \n",
    "        try:\n",
    "            data = q_in.get()            \n",
    "            in_dir = write_input_file(data)\n",
    "            logging.info('Start Job {}'.format(in_dir))\n",
    "            create_job(in_dir, image='calorbuild')   \n",
    "            result = read_output_file(in_dir)\n",
    "            q_out.put(( data, result ))   \n",
    "        except:\n",
    "            logging.error(\"Unexpected error:\", sys.exc_info()[0])\n",
    "            logging.error(traceback.format_exc())\n",
    "            raise\n",
    "        logging.info('Job {} is done'.format(in_dir))\n",
    "    return   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(optimizer, worker, num_workers, n_calls):\n",
    "    pool = Pool(num_workers) \n",
    "    m = Manager()\n",
    "    q_in = m.Queue()\n",
    "    q_out = m.Queue()\n",
    "    pool.starmap_async(worker, [(q_in, q_out)]*num_workers)\n",
    "\n",
    "    X = optimizer.ask(n_points=num_workers)\n",
    "    for i in range(num_workers):\n",
    "        point = new_point(X[i][0])\n",
    "        q_in.put(point)\n",
    "\n",
    "    for _ in tqdm(range(n_calls-num_workers)): \n",
    "        x, y = q_out.get()\n",
    "        optimizer.tell([x[0]], y) \n",
    "        point = new_point(optimizer.ask()[0])\n",
    "        q_in.put(point)\n",
    "\n",
    "    for _ in range(num_workers):\n",
    "        x, y  = q_out.get()\n",
    "        optimizer.tell([x[0]], y)      \n",
    "\n",
    "    pool.terminate()\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "optimizer = Optimizer([[low_constraint, high_constraint]]*dim,\n",
    "                      model,\n",
    "                      n_initial_points=n_initial_points,\n",
    "                      acq_func='EI',\n",
    "                      acq_optimizer='lbfgs',\n",
    "                      random_state=None)\n",
    "\n",
    "\n",
    "optimizer = optimize(optimizer, worker, 1, 1)\n",
    "print(optimizer.Xi, optimizer.yi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
